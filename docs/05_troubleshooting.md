# üîß Troubleshooting Guide - SPTrans Pipeline

## Guia de Solu√ß√£o de Problemas Comuns

---

## üö® Problemas de Infraestrutura

### 1. Docker Compose n√£o inicia

**Sintoma:**
```bash
ERROR: Couldn't connect to Docker daemon
```

**Solu√ß√µes:**
```bash
# Verificar se Docker est√° rodando
sudo systemctl status docker

# Iniciar Docker
sudo systemctl start docker

# Adicionar usu√°rio ao grupo docker
sudo usermod -aG docker $USER
newgrp docker

# Testar
docker ps
```

---

### 2. Containers crasham ao iniciar

**Sintoma:**
```bash
Container sptrans-airflow exited with code 1
```

**Diagn√≥stico:**
```bash
# Ver logs do container
docker logs sptrans-airflow

# Ver logs em tempo real
docker logs -f sptrans-airflow

# Inspecionar container
docker inspect sptrans-airflow
```

**Solu√ß√µes comuns:**
- **Porta j√° em uso**: Alterar portas no `docker-compose.yml`
- **Mem√≥ria insuficiente**: Aumentar mem√≥ria do Docker (Settings > Resources)
- **Vari√°veis de ambiente faltando**: Verificar arquivo `.env`

---

### 3. Erro de permiss√£o em volumes

**Sintoma:**
```bash
PermissionError: [Errno 13] Permission denied: '/opt/airflow/logs'
```

**Solu√ß√£o:**
```bash
# Criar diret√≥rios com permiss√µes corretas
mkdir -p logs/airflow logs/spark data/gtfs
chmod -R 777 logs data

# Ou usar o script de setup
./scripts/setup.sh
```

---

## üåê Problemas de Rede

### 4. Containers n√£o se comunicam

**Sintoma:**
```
Could not connect to PostgreSQL at postgres:5432
```

**Diagn√≥stico:**
```bash
# Verificar rede Docker
docker network ls
docker network inspect sptrans-network

# Testar conectividade
docker exec sptrans-airflow ping postgres
docker exec sptrans-airflow nc -zv postgres 5432
```

**Solu√ß√£o:**
```bash
# Recriar rede
docker network rm sptrans-network
docker network create sptrans-network

# Reiniciar containers
docker-compose down
docker-compose up -d
```

---

### 5. API SPTrans n√£o responde

**Sintoma:**
```python
requests.exceptions.ConnectionError: Failed to establish connection
```

**Diagn√≥stico:**
```bash
# Testar conectividade
curl -X POST "http://api.olhovivo.sptrans.com.br/v2.1/Login/Autenticar?token=SEU_TOKEN"

# Verificar DNS
nslookup api.olhovivo.sptrans.com.br

# Testar com timeout maior
curl --max-time 30 "http://api.olhovivo.sptrans.com.br/v2.1/Posicao"
```

**Solu√ß√µes:**
1. **Token inv√°lido**: Verificar token no portal SPTrans
2. **Firewall bloqueando**: Liberar porta 80/443
3. **Rate limit**: Aguardar alguns minutos
4. **API fora do ar**: Verificar status no portal SPTrans

---

## üíæ Problemas com MinIO (Data Lake)

### 6. MinIO n√£o inicia

**Sintoma:**
```
Unable to connect to MinIO at minio:9000
```

**Diagn√≥stico:**
```bash
# Verificar status
docker ps | grep minio

# Ver logs
docker logs minio

# Testar acesso
curl http://localhost:9000/minio/health/live
```

**Solu√ß√£o:**
```bash
# Verificar credenciais no .env
MINIO_ROOT_USER=admin
MINIO_ROOT_PASSWORD=miniopassword123

# Recriar volume se necess√°rio
docker-compose down -v
docker volume rm sptrans-minio-data
docker-compose up -d minio
```

---

### 7. Erro ao criar bucket

**Sintoma:**
```python
S3Error: Access Denied
```

**Solu√ß√£o:**
```bash
# Acessar MinIO Console
# http://localhost:9001
# Usu√°rio: admin / Senha: miniopassword123

# Criar bucket manualmente ou via script
docker exec minio mc mb minio/sptrans-datalake
docker exec minio mc policy set public minio/sptrans-datalake
```

---

## ‚ö° Problemas com Spark

### 8. Spark job falha com OutOfMemory

**Sintoma:**
```
java.lang.OutOfMemoryError: Java heap space
```

**Solu√ß√£o 1 - Aumentar mem√≥ria do executor:**
```python
# No c√≥digo Spark
spark = SparkSession.builder \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.memory", "2g") \
    .getOrCreate()
```

**Solu√ß√£o 2 - Aumentar no docker-compose:**
```yaml
spark-master:
  environment:
    - SPARK_EXECUTOR_MEMORY=4G
    - SPARK_DRIVER_MEMORY=2G
  deploy:
    resources:
      limits:
        memory: 8G
```

---

### 9. Spark n√£o conecta ao MinIO

**Sintoma:**
```
java.io.IOException: No FileSystem for scheme: s3a
```

**Solu√ß√£o:**
```python
# Adicionar JARs do Hadoop-AWS
spark = SparkSession.builder \
    .config("spark.jars.packages", 
            "org.apache.hadoop:hadoop-aws:3.3.4,"
            "com.amazonaws:aws-java-sdk-bundle:1.12.262") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "admin") \
    .config("spark.hadoop.fs.s3a.secret.key", "miniopassword123") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .getOrCreate()
```

---

## üîÑ Problemas com Airflow

### 10. DAGs n√£o aparecem na UI

**Sintoma:**
```
No DAGs found in /opt/airflow/dags
```

**Diagn√≥stico:**
```bash
# Verificar montagem do volume
docker exec sptrans-airflow ls -la /opt/airflow/dags

# Verificar parsing de DAGs
docker exec sptrans-airflow airflow dags list

# Ver erros de parsing
docker logs sptrans-airflow | grep -i error
```

**Solu√ß√£o:**
```bash
# Verificar sintaxe Python
python dags/dag_02_api_ingestion.py

# For√ßar reload de DAGs
docker exec sptrans-airflow airflow dags reserialize

# Reiniciar scheduler
docker-compose restart airflow-scheduler
```

---

### 11. Task fica preso em "running"

**Sintoma:**
```
Task has been running for more than 1 hour
```

**Diagn√≥stico:**
```bash
# Ver logs da task
docker exec sptrans-airflow airflow tasks logs dag_02_api_ingestion ingest_api_positions 2025-01-20

# Verificar processos
docker exec sptrans-airflow ps aux | grep python
```

**Solu√ß√£o:**
```bash
# Matar task travada
docker exec sptrans-airflow airflow tasks clear dag_02_api_ingestion -t ingest_api_positions

# Aumentar timeout no DAG
default_args = {
    'execution_timeout': timedelta(hours=2)
}
```

---

## üóÑÔ∏è Problemas com PostgreSQL

### 12. N√£o consegue conectar ao banco

**Sintoma:**
```
psql: error: connection to server at "postgres" (172.18.0.2), port 5432 failed
```

**Diagn√≥stico:**
```bash
# Verificar se est√° rodando
docker ps | grep postgres

# Testar conex√£o
docker exec -it postgres psql -U airflow -d airflow

# Ver logs
docker logs postgres
```

**Solu√ß√£o:**
```bash
# Recriar container
docker-compose down postgres
docker volume rm sptrans-postgres-data
docker-compose up -d postgres

# Aguardar inicializa√ß√£o
sleep 10

# Executar scripts SQL
docker exec -i postgres psql -U airflow -d airflow < sql/01_serving_schema.sql
```

---

### 13. Tabela n√£o existe

**Sintoma:**
```sql
ERROR: relation "serving.bus_positions" does not exist
```

**Solu√ß√£o:**
```bash
# Executar scripts de cria√ß√£o na ordem
cd sql/
docker exec -i postgres psql -U airflow -d airflow < 00_create_databases.sql
docker exec -i postgres psql -U airflow -d airflow < 01_serving_schema.sql
docker exec -i postgres psql -U airflow -d airflow < 02_serving_tables.sql

# Verificar tabelas criadas
docker exec -it postgres psql -U airflow -d airflow -c "\dt serving.*"
```

---

## üêç Problemas com Python

### 14. M√≥dulo n√£o encontrado

**Sintoma:**
```python
ModuleNotFoundError: No module named 'pyspark'
```

**Solu√ß√£o:**
```bash
# Instalar depend√™ncias
pip install -r requirements.txt

# Ou no container
docker exec sptrans-airflow pip install -r /opt/airflow/requirements.txt

# Reconstruir imagem
docker-compose build --no-cache
```

---

### 15. Erro de importa√ß√£o circular

**Sintoma:**
```python
ImportError: cannot import name 'SPTransClient' from partially initialized module
```

**Solu√ß√£o:**
```python
# Reorganizar imports
# Evitar:
from src.ingestion.sptrans_api_client import SPTransClient

# Preferir:
import src.ingestion.sptrans_api_client as api_client
client = api_client.SPTransClient(token)
```

---

## üìä Problemas de Dados

### 16. Dados duplicados no Data Lake

**Sintoma:**
```
Duplicate key violation in bronze layer
```

**Solu√ß√£o:**
```python
# Adicionar deduplica√ß√£o
df = df.dropDuplicates(['vehicle_id', 'timestamp'])

# Ou usar Delta Lake com merge
df.write \
    .format("delta") \
    .mode("merge") \
    .option("mergeSchema", "true") \
    .save("s3a://sptrans-datalake/silver/positions/")
```

---

### 17. Coordenadas inv√°lidas

**Sintoma:**
```
Latitude/Longitude outside S√£o Paulo bounds
```

**Solu√ß√£o:**
```python
# Validar coordenadas
def is_valid_coordinate(lat, lon):
    # Limites aproximados de S√£o Paulo
    return (-24.0 <= lat <= -23.0) and (-47.0 <= lon <= -46.0)

# Filtrar dados
df = df.filter(
    (df.latitude >= -24.0) & (df.latitude <= -23.0) &
    (df.longitude >= -47.0) & (df.longitude <= -46.0)
)
```

---

## üîç Comandos √öteis para Debug

### Verificar status geral
```bash
# Status de todos containers
docker-compose ps

# Uso de recursos
docker stats

# Logs de todos servi√ßos
docker-compose logs -f
```

### Acessar containers
```bash
# Airflow
docker exec -it sptrans-airflow bash

# Spark
docker exec -it spark-master bash

# PostgreSQL
docker exec -it postgres psql -U airflow

# MinIO
docker exec -it minio bash
```

### Limpar tudo e recome√ßar
```bash
# CUIDADO: Remove todos dados!
docker-compose down -v
docker system prune -a
rm -rf logs/* data/*

# Recriar do zero
./scripts/setup.sh
docker-compose up -d
```

---

## üìû Suporte

### Canais de Suporte
- **Issues GitHub**: Para bugs e features
- **Email SPTrans**: desenvolvedores@sptrans.com.br (problemas com API)
- **Documenta√ß√£o**: Consultar `/docs` do projeto

### Logs Importantes
1. `/logs/airflow/scheduler/*.log` - Airflow scheduler
2. `/logs/spark/*.log` - Jobs Spark
3. `docker logs <container>` - Logs dos containers

---

**√öltima atualiza√ß√£o**: Janeiro 2025
