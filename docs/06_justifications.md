# ğŸ¯ Justificativas TÃ©cnicas das Escolhas Arquiteturais

> **Documento para AvaliaÃ§Ã£o AcadÃªmica - FIA/LABDATA**  
> **Respostas aos Questionamentos do Professor**

---

## ğŸ“‹ Ãndice

1. [Resposta aos ComentÃ¡rios do Professor](#1-resposta-aos-comentÃ¡rios-do-professor)
2. [Justificativa: MinIO como Data Lake](#2-justificativa-minio-como-data-lake)
3. [Justificativa: Spark para IngestÃ£o](#3-justificativa-spark-para-ingestÃ£o)
4. [Justificativa: Arquitetura Medallion](#4-justificativa-arquitetura-medallion)
5. [Justificativa: Tecnologias Open Source](#5-justificativa-tecnologias-open-source)
6. [Trade-offs e DecisÃµes](#6-trade-offs-e-decisÃµes)
7. [ComparaÃ§Ã£o com Alternativas](#7-comparaÃ§Ã£o-com-alternativas)

---

## 1. Resposta aos ComentÃ¡rios do Professor

### 1.1. "Preciso do desenho do fluxo de informaÃ§Ãµes detalhado"

**âœ… RESPONDIDO**

O fluxo completo estÃ¡ documentado em [`01_architecture.md`](./01_architecture.md) com os seguintes nÃ­veis de detalhamento:

1. **Diagrama de Alto NÃ­vel** (7 camadas)
2. **Fluxo de Dados por Etapa** (4 transformaÃ§Ãµes principais)
3. **EspecificaÃ§Ã£o de cada Spark Job** (inputs, processing, outputs)
4. **DependÃªncias entre DAGs** (grafo de execuÃ§Ã£o)
5. **Schemas de dados** (entrada e saÃ­da de cada camada)

**Resumo Visual:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FLUXO DETALHADO DE INFORMAÃ‡Ã•ES                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  [1] API SPTrans (JSON, 2 min) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚                                                      â”‚              â”‚
â”‚  [2] GTFS SPTrans (CSV, diÃ¡rio) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚              â”‚
â”‚                                                  â”‚   â”‚              â”‚
â”‚                                                  â–¼   â–¼              â”‚
â”‚  [3] Airflow Scheduler â”€â”€â”€â”€> Trigger Spark Jobs                    â”‚
â”‚                                                  â”‚                  â”‚
â”‚                                                  â–¼                  â”‚
â”‚  [4] Spark Job: API Ingestion                                      â”‚
â”‚      Input: HTTP REST API                                          â”‚
â”‚      Process:                                                      â”‚
â”‚        â€¢ AutenticaÃ§Ã£o (Bearer Token)                               â”‚
â”‚        â€¢ GET /Posicao                                              â”‚
â”‚        â€¢ Parse JSON (15k registros)                                â”‚
â”‚        â€¢ Schema validation (PySpark StructType)                    â”‚
â”‚        â€¢ Add metadata (ingestion_timestamp, pipeline_version)      â”‚
â”‚      Output: Parquet partitioned by date/hour                      â”‚
â”‚      Path: s3a://sptrans-bronze/api_positions/year=.../           â”‚
â”‚                                                  â”‚                  â”‚
â”‚                                                  â–¼                  â”‚
â”‚  [5] MinIO Storage (Bronze Layer)                                  â”‚
â”‚      Format: Apache Parquet (columnar)                             â”‚
â”‚      Compression: Snappy                                           â”‚
â”‚      Partitioning: Hive-style (year/month/day/hour)               â”‚
â”‚      Size: ~50GB/month                                             â”‚
â”‚                                                  â”‚                  â”‚
â”‚                                                  â–¼                  â”‚
â”‚  [6] Spark Job: Bronze â†’ Silver                                    â”‚
â”‚      Input: Parquet files (last 1 hour)                            â”‚
â”‚      Process:                                                      â”‚
â”‚        â€¢ Data Quality Checks:                                      â”‚
â”‚          - Check nulls (vehicle_id, lat, lon, timestamp)          â”‚
â”‚          - Validate coordinates (-90<lat<90, -180<lon<180)        â”‚
â”‚          - Check future timestamps                                 â”‚
â”‚        â€¢ Deduplication:                                            â”‚
â”‚          - Window function: ROW_NUMBER() OVER (                    â”‚
â”‚              PARTITION BY vehicle_id, timestamp                    â”‚
â”‚              ORDER BY ingestion_timestamp DESC)                    â”‚
â”‚        â€¢ Enrichment:                                               â”‚
â”‚          - Join GTFS routes (by route_short_name)                 â”‚
â”‚          - Spatial join stops (ST_Distance < 50m)                 â”‚
â”‚          - Geocoding reverso: Nominatim API                        â”‚
â”‚            (lat, lon) â†’ (street, neighborhood, district)          â”‚
â”‚        â€¢ Derived fields:                                           â”‚
â”‚          - speed_kmh = distance/time (Haversine formula)          â”‚
â”‚          - heading_degrees = ATAN2(Î”lat, Î”lon)                    â”‚
â”‚          - is_moving = speed > 5 km/h                              â”‚
â”‚      Output: Delta Lake table (ACID, versioned)                    â”‚
â”‚      Path: s3a://sptrans-silver/positions_cleaned/                â”‚
â”‚                                                  â”‚                  â”‚
â”‚                                                  â–¼                  â”‚
â”‚  [7] Delta Lake Storage (Silver Layer)                             â”‚
â”‚      Format: Delta Lake (Parquet + transaction log)                â”‚
â”‚      Features: MERGE, Time Travel, Schema Evolution                â”‚
â”‚      Size: ~30GB/month (40% reduction via DQ)                      â”‚
â”‚                                                  â”‚                  â”‚
â”‚                                                  â–¼                  â”‚
â”‚  [8] Spark Job: Silver â†’ Gold                                      â”‚
â”‚      Input: Silver Delta Lake (last 24 hours)                      â”‚
â”‚      Process:                                                      â”‚
â”‚        â€¢ KPI Calculations:                                         â”‚
â”‚          a) Speed Analysis:                                        â”‚
â”‚             SELECT route_id, hour,                                 â”‚
â”‚                    AVG(speed_kmh) as avg_speed,                    â”‚
â”‚                    PERCENTILE(speed_kmh, 0.5) as median_speed,    â”‚
â”‚                    STDDEV(speed_kmh) as speed_stddev              â”‚
â”‚             GROUP BY route_id, hour                                â”‚
â”‚                                                                     â”‚
â”‚          b) Headway Calculation:                                   â”‚
â”‚             SELECT route_id, stop_id,                              â”‚
â”‚                    AVG(LEAD(timestamp) - timestamp) as headway    â”‚
â”‚             OVER (PARTITION BY route_id, stop_id)                 â”‚
â”‚                                                                     â”‚
â”‚          c) Punctuality:                                           â”‚
â”‚             SELECT trip_id,                                        â”‚
â”‚                    actual_arrival - scheduled_arrival as delay,   â”‚
â”‚                    CASE WHEN delay < 5 THEN 'on_time'            â”‚
â”‚                         ELSE 'delayed' END as status              â”‚
â”‚             FROM positions JOIN gtfs_stop_times                    â”‚
â”‚                                                                     â”‚
â”‚          d) Anomaly Detection:                                     â”‚
â”‚             z_score = (speed - AVG(speed)) / STDDEV(speed)        â”‚
â”‚             WHERE ABS(z_score) > 3                                 â”‚
â”‚      Output: Delta Lake aggregated tables                          â”‚
â”‚      Path: s3a://sptrans-gold/kpis_*/                             â”‚
â”‚                                                  â”‚                  â”‚
â”‚                                                  â–¼                  â”‚
â”‚  [9] Delta Lake Storage (Gold Layer)                               â”‚
â”‚      Tables: kpis_realtime, metrics_by_route, headway_analysis    â”‚
â”‚      Size: ~2GB/month (99% reduction via aggregation)             â”‚
â”‚                                                  â”‚                  â”‚
â”‚                                                  â–¼                  â”‚
â”‚  [10] Spark Job: Gold â†’ PostgreSQL                                 â”‚
â”‚       Input: Gold Delta Lake                                       â”‚
â”‚       Process:                                                     â”‚
â”‚         â€¢ Read aggregated data                                     â”‚
â”‚         â€¢ Round values (2 decimal places)                          â”‚
â”‚         â€¢ Convert timestamps to America/Sao_Paulo timezone         â”‚
â”‚         â€¢ Filter last 7 days only                                  â”‚
â”‚       Output: PostgreSQL UPSERT                                    â”‚
â”‚         INSERT INTO serving.kpis_realtime                          â”‚
â”‚         ON CONFLICT (route_id, timestamp)                          â”‚
â”‚         DO UPDATE SET ...                                          â”‚
â”‚                                                  â”‚                  â”‚
â”‚                                                  â–¼                  â”‚
â”‚  [11] PostgreSQL (Serving Layer)                                   â”‚
â”‚       Tables: kpis_realtime, route_metrics, vehicle_status         â”‚
â”‚       Views: v_dashboard_summary, v_route_performance              â”‚
â”‚       Size: ~5GB (7 days rolling window)                           â”‚
â”‚                                                  â”‚                  â”‚
â”‚                                              â”Œâ”€â”€â”€â”´â”€â”€â”€â”              â”‚
â”‚                                              â–¼       â–¼              â”‚
â”‚  [12] Visualization Layer                                          â”‚
â”‚       â€¢ Apache Superset (BI dashboards)                            â”‚
â”‚       â€¢ Grafana (monitoring)                                       â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 1.2. "O projeto foi feito inicialmente usando PostgreSQL como lake quando Ã© necessÃ¡rio um lake com maior capacidade de volumetria"

**âœ… CORRIGIDO - JUSTIFICATIVA**

#### Problema Identificado

PostgreSQL **NÃƒO** Ã© adequado como Data Lake para dados de alta volumetria porque:

1. **Custo de Storage**: PostgreSQL armazena dados em formato row-oriented (otimizado para OLTP, nÃ£o analytics)
2. **Escalabilidade**: Requer particionamento manual e sharding complexo
3. **Performance**: Queries analÃ­ticos sÃ£o lentos em tabelas com bilhÃµes de registros
4. **Volumetria**: 5.4M registros/dia = ~200M registros/mÃªs = ~500GB no PostgreSQL

#### SoluÃ§Ã£o Implementada: MinIO como Data Lake

**Por que MinIO?**

| CritÃ©rio | PostgreSQL | MinIO + Parquet | Vantagem |
|----------|-----------|-----------------|----------|
| **Storage Format** | Row-oriented | Columnar (Parquet) | âœ… 10x compressÃ£o |
| **Query Performance** | Seq scan = lento | Predicate pushdown | âœ… 50x mais rÃ¡pido |
| **Custo Storage** | $0.10/GB/mÃªs | $0.02/GB/mÃªs | âœ… 5x mais barato |
| **Escalabilidade** | Vertical (1 server) | Horizontal (cluster) | âœ… Infinita |
| **Compatibilidade** | SQL only | S3 API (universal) | âœ… Spark, Presto, Athena |
| **Backup** | pg_dump (lento) | Object copy (rÃ¡pido) | âœ… Snapshots instantÃ¢neos |

**CÃ¡lculo de Volumetria:**

```
Dados da API SPTrans:
â€¢ 15.000 veÃ­culos
â€¢ AtualizaÃ§Ã£o a cada 2 minutos
â€¢ 720 coletas/dia
â€¢ 15.000 Ã— 720 = 10.8M registros/dia

Tamanho por registro (JSON):
â€¢ ~200 bytes/registro

Volume diÃ¡rio (PostgreSQL):
â€¢ 10.8M Ã— 200 bytes = 2.16GB/dia RAW
â€¢ Com Ã­ndices: ~4GB/dia
â€¢ 30 dias: 120GB
â€¢ 90 dias: 360GB

Volume diÃ¡rio (MinIO Parquet + Snappy):
â€¢ Parquet columnar: 50% de compressÃ£o
â€¢ Snappy: mais 50% de compressÃ£o
â€¢ Total: 75% de reduÃ§Ã£o
â€¢ 2.16GB Ã— 0.25 = 540MB/dia
â€¢ 30 dias: 16GB
â€¢ 90 dias: 49GB

ECONOMIA: 360GB â†’ 49GB (86% de reduÃ§Ã£o)
```

**PostgreSQL no Projeto Corrigido:**

âœ… **Usado corretamente** como **Serving Layer** (nÃ£o Data Lake):
- Armazena apenas dados agregados (Gold Layer)
- Volume: ~5GB (7 dias rolling window)
- Uso: Queries rÃ¡pidas para dashboards
- Views materializadas para cache

#### Arquitetura Final Corrigida

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ARQUITETURA CORRIGIDA                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                 â”‚
â”‚  â”‚ API SPTrans â”‚ (15k vehicles, 2 min frequency)                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                 â”‚
â”‚         â”‚                                                         â”‚
â”‚         â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚  â”‚ Apache Spark    â”‚ â† IngestÃ£o distribuÃ­da (nÃ£o Python puro)   â”‚
â”‚  â”‚ Batch Job       â”‚                                             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚           â”‚                                                       â”‚
â”‚           â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ MinIO (Data Lake) - S3 Compatible                