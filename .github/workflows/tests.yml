name: Tests - Automated Testing

on:
  push:
    branches:
      - main
      - develop
      - feature/*
  pull_request:
    branches:
      - main
      - develop
  schedule:
    # Run tests every day at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.9'
  SPARK_VERSION: '3.4.0'
  HADOOP_VERSION: '3'

jobs:
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        python-version: ['3.8', '3.9', '3.10']
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-xdist pytest-timeout pytest-mock
      
      - name: Run unit tests
        run: |
          pytest tests/unit/ \
            -v \
            --cov=src \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term-missing \
            --junitxml=junit/test-results-${{ matrix.python-version }}.xml \
            --maxfail=5 \
            --timeout=300 \
            -n auto
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage.xml
          flags: unittests
          name: codecov-unit-${{ matrix.python-version }}
          fail_ci_if_error: false
      
      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: unit-test-results-${{ matrix.python-version }}
          path: |
            junit/test-results-*.xml
            htmlcov/
      
      - name: Publish test results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          files: |
            junit/test-results-*.xml

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: sptrans_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      minio:
        image: minio/minio:latest
        ports:
          - 9000:9000
          - 9001:9001
        env:
          MINIO_ROOT_USER: minioadmin
          MINIO_ROOT_PASSWORD: minioadmin
        options: >-
          --health-cmd "curl -f http://localhost:9000/minio/health/live"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        command: server /data --console-address ":9001"
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-timeout
      
      - name: Wait for services
        run: |
          timeout 60 bash -c 'until pg_isready -h localhost -p 5432; do sleep 1; done'
          timeout 60 bash -c 'until redis-cli -h localhost -p 6379 ping; do sleep 1; done'
          echo "All services are ready"
      
      - name: Setup test database
        env:
          POSTGRES_HOST: localhost
          POSTGRES_PORT: 5432
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: sptrans_test
        run: |
          psql -h localhost -U test_user -d sptrans_test -f sql/00_create_databases.sql || true
          psql -h localhost -U test_user -d sptrans_test -f sql/01_serving_schema.sql || true
          psql -h localhost -U test_user -d sptrans_test -f sql/02_serving_tables.sql || true
      
      - name: Run integration tests
        env:
          POSTGRES_HOST: localhost
          POSTGRES_PORT: 5432
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: sptrans_test
          REDIS_HOST: localhost
          REDIS_PORT: 6379
          MINIO_ENDPOINT: localhost:9000
          MINIO_ACCESS_KEY: minioadmin
          MINIO_SECRET_KEY: minioadmin
        run: |
          pytest tests/integration/ \
            -v \
            --cov=src \
            --cov-append \
            --cov-report=xml \
            --junitxml=junit/test-integration-results.xml \
            --timeout=600
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage.xml
          flags: integrationtests
          name: codecov-integration
      
      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: integration-test-results
          path: junit/test-integration-results.xml

  spark-tests:
    name: Spark Jobs Tests
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Set up Java
        uses: actions/setup-java@v3
        with:
          distribution: 'temurin'
          java-version: '11'
      
      - name: Install PySpark
        run: |
          python -m pip install --upgrade pip
          pip install pyspark==${{ env.SPARK_VERSION }}
          pip install pytest pytest-spark
      
      - name: Install project dependencies
        run: |
          pip install -r requirements.txt
      
      - name: Run Spark job tests
        env:
          SPARK_LOCAL_IP: 127.0.0.1
        run: |
          pytest tests/unit/ tests/integration/ -k spark \
            -v \
            --junitxml=junit/test-spark-results.xml \
            --timeout=900
      
      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: spark-test-results
          path: junit/test-spark-results.xml

  airflow-tests:
    name: Airflow DAGs Tests
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install apache-airflow==2.7.0
          pip install pytest pytest-airflow
          pip install -r requirements.txt
      
      - name: Initialize Airflow DB
        run: |
          airflow db init
      
      - name: Validate DAG integrity
        run: |
          python -c "
          import sys
          from pathlib import Path
          sys.path.insert(0, str(Path.cwd()))
          
          from airflow.models import DagBag
          
          dag_bag = DagBag(dag_folder='dags/', include_examples=False)
          
          if dag_bag.import_errors:
              print('DAG Import Errors:')
              for filename, error in dag_bag.import_errors.items():
                  print(f'{filename}: {error}')
              sys.exit(1)
          
          print(f'Successfully loaded {len(dag_bag.dags)} DAGs')
          
          # Check for cyclic dependencies
          for dag_id, dag in dag_bag.dags.items():
              dag.test_cycle()
              print(f'âœ“ {dag_id}: No cycles detected')
          "
      
      - name: Run DAG tests
        run: |
          pytest tests/unit/ tests/integration/ -k dag \
            -v \
            --junitxml=junit/test-airflow-results.xml \
            --timeout=600
      
      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: airflow-test-results
          path: junit/test-airflow-results.xml

  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Start services with docker-compose
        run: |
          docker-compose up -d
          sleep 60  # Wait for services to be ready
      
      - name: Check services health
        run: |
          docker-compose ps
          docker-compose logs --tail=50
      
      - name: Run end-to-end tests
        run: |
          docker-compose exec -T airflow pytest tests/integration/test_end_to_end.py -v || true
      
      - name: Collect logs
        if: always()
        run: |
          mkdir -p logs
          docker-compose logs > logs/docker-compose.log
      
      - name: Upload logs
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: e2e-logs
          path: logs/
      
      - name: Cleanup
        if: always()
        run: |
          docker-compose down -v

  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-benchmark locust
      
      - name: Run performance tests
        run: |
          pytest tests/unit/ -k performance --benchmark-only \
            --benchmark-json=benchmark.json
      
      - name: Store benchmark results
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'pytest'
          output-file-path: benchmark.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true

  test-report:
    name: Generate Test Report
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, spark-tests, airflow-tests]
    if: always()
    
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v3
      
      - name: Generate combined report
        run: |
          mkdir -p test-reports
          echo "# Test Report" > test-reports/REPORT.md
          echo "" >> test-reports/REPORT.md
          echo "## Summary" >> test-reports/REPORT.md
          echo "- Unit Tests: ${{ needs.unit-tests.result }}" >> test-reports/REPORT.md
          echo "- Integration Tests: ${{ needs.integration-tests.result }}" >> test-reports/REPORT.md
          echo "- Spark Tests: ${{ needs.spark-tests.result }}" >> test-reports/REPORT.md
          echo "- Airflow Tests: ${{ needs.airflow-tests.result }}" >> test-reports/REPORT.md
      
      - name: Upload combined report
        uses: actions/upload-artifact@v3
        with:
          name: test-report
          path: test-reports/
      
      - name: Comment PR with results
        uses: actions/github-script@v6
        if: github.event_name == 'pull_request'
        with:
          script: |
            const output = `
            #### Test Results ðŸ§ª
            - Unit Tests: \`${{ needs.unit-tests.result }}\`
            - Integration Tests: \`${{ needs.integration-tests.result }}\`
            - Spark Tests: \`${{ needs.spark-tests.result }}\`
            - Airflow Tests: \`${{ needs.airflow-tests.result }}\`
            
            *Workflow: \`${{ github.workflow }}\`*
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: output
            })

  notify:
    name: Notify Test Results
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, spark-tests, airflow-tests, e2e-tests]
    if: always()
    
    steps:
      - name: Send Slack notification
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: |
            Test Results:
            - Unit: ${{ needs.unit-tests.result }}
            - Integration: ${{ needs.integration-tests.result }}
            - Spark: ${{ needs.spark-tests.result }}
            - Airflow: ${{ needs.airflow-tests.result }}
            - E2E: ${{ needs.e2e-tests.result }}
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
        if: always()
