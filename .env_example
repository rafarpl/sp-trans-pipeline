# ============================================================================
# SPTRANS REAL-TIME PIPELINE - ENVIRONMENT VARIABLES
# ============================================================================
# Template de variáveis de ambiente
# Copie este arquivo para .env e preencha com seus valores
# Comando: cp .env.example .env
# ============================================================================

# ============================================================================
# ENVIRONMENT
# ============================================================================
ENVIRONMENT=development
# Valores possíveis: development | staging | production

LOG_LEVEL=INFO
# Valores possíveis: DEBUG | INFO | WARNING | ERROR | CRITICAL

TZ=America/Sao_Paulo
# Timezone para logs e timestamps

PROJECT_NAME=sptrans-realtime-pipeline
PROJECT_VERSION=1.0.0

# ============================================================================
# SPTRANS API (Olho Vivo)
# ============================================================================
SPTRANS_API_TOKEN=seu_token_aqui
# Obter em: https://www.sptrans.com.br/desenvolvedores/
# OBRIGATÓRIO: Substitua com seu token real

SPTRANS_API_BASE_URL=https://api.olhovivo.sptrans.com.br/v2.1
SPTRANS_API_TIMEOUT=30
# Timeout em segundos para requisições

SPTRANS_API_MAX_RETRIES=3
# Número máximo de tentativas em caso de falha

SPTRANS_API_RETRY_DELAY=5
# Delay em segundos entre retries

# ============================================================================
# MINIO (Data Lake - S3 Compatible)
# ============================================================================
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin123
# ATENÇÃO: ALTERAR EM PRODUÇÃO!

MINIO_ENDPOINT=minio:9000
MINIO_ENDPOINT_EXTERNAL=localhost:9000
# Endpoint para acesso de fora do Docker

MINIO_CONSOLE_PORT=9001
MINIO_USE_SSL=false
# Em produção, use true com certificados válidos

# Buckets
MINIO_BUCKET_BRONZE=sptrans-bronze
MINIO_BUCKET_SILVER=sptrans-silver
MINIO_BUCKET_GOLD=sptrans-gold

# Configurações S3
AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
AWS_DEFAULT_REGION=us-east-1
S3_ENDPOINT_URL=http://${MINIO_ENDPOINT}

# ============================================================================
# POSTGRESQL (Data Warehouse + Airflow Metadata)
# ============================================================================
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=airflow
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow123
# ATENÇÃO: ALTERAR EM PRODUÇÃO!

# Database SPTrans (Data Warehouse)
POSTGRES_DB_SPTRANS=sptrans
POSTGRES_SCHEMA_SERVING=serving
POSTGRES_SCHEMA_METADATA=metadata

# Configurações de Performance
POSTGRES_MAX_CONNECTIONS=200
POSTGRES_SHARED_BUFFERS=2GB
POSTGRES_EFFECTIVE_CACHE_SIZE=6GB
POSTGRES_WORK_MEM=256MB
POSTGRES_MAINTENANCE_WORK_MEM=1GB

# ============================================================================
# REDIS (Cache + Celery Broker)
# ============================================================================
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=redis123
# ATENÇÃO: ALTERAR EM PRODUÇÃO!

REDIS_DB=0
REDIS_MAX_MEMORY=2gb
REDIS_EVICTION_POLICY=allkeys-lru

# ============================================================================
# APACHE AIRFLOW
# ============================================================================
AIRFLOW_UID=50000
AIRFLOW_GID=0

# Core
AIRFLOW__CORE__EXECUTOR=CeleryExecutor
AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
AIRFLOW__CORE__LOAD_EXAMPLES=false
AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=false
AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG=3
AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=60
AIRFLOW__CORE__DEFAULT_TIMEZONE=America/Sao_Paulo

# Fernet Key (criptografia de secrets)
AIRFLOW__CORE__FERNET_KEY=ZmDfcTF7_60GrrY167zsiPd67pEvs0aGOv2oasOM1Pg=
# ATENÇÃO: GERAR NOVO EM PRODUÇÃO!
# Comando: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"

# Webserver
AIRFLOW__WEBSERVER__SECRET_KEY=sua_secret_key_aqui_minimo_16_chars
# ATENÇÃO: GERAR NOVO EM PRODUÇÃO!
# Comando: openssl rand -hex 16

AIRFLOW__WEBSERVER__BASE_URL=http://localhost:8080
AIRFLOW__WEBSERVER__WEB_SERVER_PORT=8080
AIRFLOW__WEBSERVER__WORKERS=4
AIRFLOW__WEBSERVER__EXPOSE_CONFIG=true

# Celery (Distributed Task Queue)
AIRFLOW__CELERY__BROKER_URL=redis://:${REDIS_PASSWORD}@${REDIS_HOST}:${REDIS_PORT}/0
AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}
AIRFLOW__CELERY__WORKER_CONCURRENCY=4

# Scheduler
AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT=false
AIRFLOW__SCHEDULER__MAX_TIS_PER_QUERY=512
AIRFLOW__SCHEDULER__SCHEDULE_AFTER_TASK_EXECUTION=true

# Logging
AIRFLOW__LOGGING__BASE_LOG_FOLDER=/opt/airflow/logs
AIRFLOW__LOGGING__LOGGING_LEVEL=INFO
AIRFLOW__LOGGING__FAB_LOGGING_LEVEL=WARNING
AIRFLOW__LOGGING__REMOTE_LOGGING=false

# Email (Alertas)
AIRFLOW__EMAIL__EMAIL_BACKEND=airflow.utils.email.send_email_smtp
AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com
AIRFLOW__SMTP__SMTP_STARTTLS=true
AIRFLOW__SMTP__SMTP_SSL=false
AIRFLOW__SMTP__SMTP_PORT=587
AIRFLOW__SMTP__SMTP_MAIL_FROM=airflow@sptrans.local
AIRFLOW__SMTP__SMTP_USER=seu_email@gmail.com
AIRFLOW__SMTP__SMTP_PASSWORD=sua_senha_app_gmail
# Para Gmail: Gere senha de app em https://myaccount.google.com/apppasswords

# ============================================================================
# APACHE SPARK
# ============================================================================
SPARK_VERSION=3.5.0
SPARK_MASTER_HOST=spark-master
SPARK_MASTER_PORT=7077
SPARK_MASTER_WEBUI_PORT=8081

# Worker Configuration
SPARK_WORKER_CORES=2
SPARK_WORKER_MEMORY=4g
SPARK_WORKER_INSTANCES=2

# Executor Configuration
SPARK_EXECUTOR_MEMORY=4g
SPARK_EXECUTOR_CORES=2
SPARK_DRIVER_MEMORY=2g

# Spark Defaults
SPARK_SQL_SHUFFLE_PARTITIONS=8
SPARK_DEFAULT_PARALLELISM=12
SPARK_SQL_ADAPTIVE_ENABLED=true

# Delta Lake
DELTA_VERSION=3.0.0

# Spark History Server (Opcional)
SPARK_HISTORY_SERVER_ENABLED=false
SPARK_HISTORY_SERVER_PORT=18080

# ============================================================================
# APACHE SUPERSET (Business Intelligence)
# ============================================================================
SUPERSET_SECRET_KEY=sua_superset_secret_key_minimo_42_chars
# ATENÇÃO: GERAR NOVO EM PRODUÇÃO!
# Comando: openssl rand -base64 42

SUPERSET_ADMIN_USERNAME=admin
SUPERSET_ADMIN_PASSWORD=admin123
# ATENÇÃO: ALTERAR EM PRODUÇÃO!

SUPERSET_ADMIN_FIRSTNAME=Admin
SUPERSET_ADMIN_LASTNAME=User
SUPERSET_ADMIN_EMAIL=admin@sptrans.local

SUPERSET_DATABASE_URI=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}
SUPERSET_WEBSERVER_PORT=8088
SUPERSET_ROW_LIMIT=50000

# ============================================================================
# GRAFANA (Monitoring & Alerting)
# ============================================================================
GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=admin123
# ATENÇÃO: ALTERAR EM PRODUÇÃO!

GRAFANA_PORT=3000
GRAFANA_INSTALL_PLUGINS=
# Plugins separados por vírgula, ex: grafana-clock-panel,grafana-piechart-panel

GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER}
GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
GF_USERS_ALLOW_SIGN_UP=false
GF_SERVER_ROOT_URL=http://localhost:3000

# Datasources
GF_DATABASE_TYPE=postgres
GF_DATABASE_HOST=${POSTGRES_HOST}:${POSTGRES_PORT}
GF_DATABASE_NAME=${POSTGRES_DB}
GF_DATABASE_USER=${POSTGRES_USER}
GF_DATABASE_PASSWORD=${POSTGRES_PASSWORD}

# ============================================================================
# PROMETHEUS (Metrics Collection)
# ============================================================================
PROMETHEUS_PORT=9090
PROMETHEUS_RETENTION_TIME=30d
PROMETHEUS_SCRAPE_INTERVAL=15s
PROMETHEUS_EVALUATION_INTERVAL=15s

# ============================================================================
# KAFKA (Opcional - para streaming real-time)
# ============================================================================
KAFKA_ENABLED=false
# Se true, o docker-compose vai subir Kafka + Zookeeper

KAFKA_BROKER=kafka:9092
KAFKA_ZOOKEEPER=zookeeper:2181
KAFKA_TOPIC_POSITIONS=sptrans.positions
KAFKA_TOPIC_PARTITIONS=6
KAFKA_TOPIC_REPLICATION_FACTOR=1

# ============================================================================
# DATA PIPELINE CONFIGURATION
# ============================================================================

# Ingestion
API_INGESTION_INTERVAL_MINUTES=2
GTFS_INGESTION_SCHEDULE="0 3 * * *"
# Cron: Diariamente às 3 AM

# Processing
BRONZE_TO_SILVER_INTERVAL_MINUTES=10
SILVER_TO_GOLD_INTERVAL_MINUTES=15
GOLD_TO_SERVING_INTERVAL_MINUTES=15

# Data Quality
DQ_CHECK_INTERVAL_MINUTES=60
DQ_SCORE_THRESHOLD=95.0
DQ_CRITICAL_THRESHOLD=85.0
# Se DQ Score < 85%, pipeline é bloqueado

# Retention
DATA_RETENTION_BRONZE_DAYS=90
DATA_RETENTION_SILVER_DAYS=90
DATA_RETENTION_GOLD_DAYS=365
DATA_RETENTION_SERVING_DAYS=7

# Batch Sizes
BATCH_SIZE_API_INGESTION=15000
BATCH_SIZE_BRONZE_TO_SILVER=100000
BATCH_SIZE_SILVER_TO_GOLD=50000

# ============================================================================
# GEOCODING (Opcional)
# ============================================================================
GEOCODING_ENABLED=true
GEOCODING_PROVIDER=nominatim
# Valores: nominatim | google | mapbox

# Nominatim (Open Source - Gratuito)
NOMINATIM_ENDPOINT=https://nominatim.openstreetmap.org
NOMINATIM_USER_AGENT=sptrans-pipeline/1.0

# Google Maps API (Pago)
GOOGLE_MAPS_API_KEY=sua_api_key_aqui
# Obter em: https://console.cloud.google.com/

# Mapbox API (Pago)
MAPBOX_API_KEY=sua_api_key_aqui
# Obter em: https://account.mapbox.com/

# ============================================================================
# ALERTING
# ============================================================================
ALERTING_ENABLED=true
ALERT_EMAIL_TO=alert@example.com
ALERT_EMAIL_FROM=noreply@sptrans.local

# Slack Webhook (Opcional)
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
SLACK_CHANNEL=#data-alerts

# PagerDuty (Opcional)
PAGERDUTY_ENABLED=false
PAGERDUTY_API_KEY=sua_api_key_aqui
PAGERDUTY_SERVICE_ID=seu_service_id

# ============================================================================
# MONITORING & OBSERVABILITY
# ============================================================================
ENABLE_METRICS=true
ENABLE_TRACING=false
# Distributed tracing (requer Jaeger/Zipkin)

METRICS_PORT=9090
HEALTH_CHECK_PORT=8000

# ============================================================================
# SECURITY
# ============================================================================
# SSL/TLS
SSL_ENABLED=false
SSL_CERT_PATH=/certs/cert.pem
SSL_KEY_PATH=/certs/key.pem

# Authentication
AUTH_ENABLED=false
AUTH_TYPE=basic
# Valores: basic | oauth | ldap

# Network
ALLOWED_HOSTS=localhost,127.0.0.1
CORS_ENABLED=false
CORS_ALLOWED_ORIGINS=*

# ============================================================================
# BACKUP & DISASTER RECOVERY
# ============================================================================
BACKUP_ENABLED=true
BACKUP_SCHEDULE="0 2 * * *"
# Cron: Diariamente às 2 AM

BACKUP_RETENTION_DAYS=30
BACKUP_PATH=/data/backups

# PostgreSQL Backup
POSTGRES_BACKUP_ENABLED=true
POSTGRES_BACKUP_PATH=/backups/postgres

# MinIO Backup
MINIO_BACKUP_ENABLED=true
MINIO_BACKUP_PATH=/backups/minio

# ============================================================================
# DEVELOPMENT & TESTING
# ============================================================================
DEBUG_MODE=false
# NUNCA use true em produção!

ENABLE_PROFILING=false
# Performance profiling (overhead)

RUN_TESTS_ON_STARTUP=false
# Executa testes ao iniciar containers

MOCK_API_ENABLED=false
# Usa dados mockados ao invés da API real

SAMPLE_DATA_SIZE=1000
# Quantidade de registros para dados de teste

# ============================================================================
# RESOURCE LIMITS (Docker)
# ============================================================================
# Limites de CPU e memória por serviço
# Formato: number of CPUs (0.5 = 50%, 2 = 200%)

# Airflow
AIRFLOW_WEBSERVER_CPU=1.0
AIRFLOW_WEBSERVER_MEM=2g
AIRFLOW_SCHEDULER_CPU=2.0
AIRFLOW_SCHEDULER_MEM=4g
AIRFLOW_WORKER_CPU=2.0
AIRFLOW_WORKER_MEM=4g

# Spark
SPARK_MASTER_CPU=1.0
SPARK_MASTER_MEM=2g
SPARK_WORKER_CPU=2.0
SPARK_WORKER_MEM=8g

# Databases
POSTGRES_CPU=2.0
POSTGRES_MEM=4g
REDIS_CPU=0.5
REDIS_MEM=2g

# MinIO
MINIO_CPU=1.0
MINIO_MEM=2g

# ============================================================================
# FEATURE FLAGS
# ============================================================================
FEATURE_KAFKA_STREAMING=false
FEATURE_ML_PREDICTIONS=false
FEATURE_REAL_TIME_ALERTS=true
FEATURE_ADVANCED_ANALYTICS=true

# ============================================================================
# FIM DO ARQUIVO
# ============================================================================
# Após preencher este arquivo:
# 1. Renomeie para .env: mv .env.example .env
# 2. Gere novas secret keys para produção
# 3. Substitua senhas padrão
# 4. Configure credenciais da API SPTrans
# 5. Verifique configurações de recursos (CPU/RAM)
# ============================================================================