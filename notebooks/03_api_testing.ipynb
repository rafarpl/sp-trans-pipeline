{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧪 Testes da API SPTrans - Tempo Real\n",
    "\n",
    "Este notebook realiza testes completos da API SPTrans em tempo real.\n",
    "\n",
    "## Objetivos\n",
    "1. Validar autenticação e conectividade com a API\n",
    "2. Testar todos os endpoints disponíveis\n",
    "3. Analisar estrutura e qualidade dos dados retornados\n",
    "4. Verificar performance e latência\n",
    "5. Detectar anomalias e inconsistências\n",
    "6. Documentar exemplos de resposta\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from typing import Dict, List, Optional\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configurações\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Carregar variáveis de ambiente\n",
    "load_dotenv('../.env')\n",
    "\n",
    "print(\"✅ Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ Configuração da API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações da API SPTrans\n",
    "API_TOKEN = os.getenv('SPTRANS_API_TOKEN', 'YOUR_TOKEN_HERE')\n",
    "BASE_URL = 'http://api.olhovivo.sptrans.com.br/v2.1'\n",
    "\n",
    "# Verificar token\n",
    "if API_TOKEN == 'YOUR_TOKEN_HERE':\n",
    "    print(\"⚠️ ATENÇÃO: Configure seu token da API SPTrans no arquivo .env\")\n",
    "    print(\"\\nPara obter um token:\")\n",
    "    print(\"1. Acesse: http://www.sptrans.com.br/desenvolvedores/\")\n",
    "    print(\"2. Cadastre-se e solicite um token de acesso\")\n",
    "    print(\"3. Adicione o token no arquivo .env: SPTRANS_API_TOKEN=seu_token_aqui\")\n",
    "else:\n",
    "    print(f\"✅ Token configurado: {API_TOKEN[:10]}...{API_TOKEN[-10:]}\")\n",
    "    print(f\"✅ Base URL: {BASE_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe para interagir com a API\n",
    "class SPTransAPITester:\n",
    "    \"\"\"Classe para testar a API SPTrans\"\"\"\n",
    "    \n",
    "    def __init__(self, token: str, base_url: str):\n",
    "        self.token = token\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "        self.authenticated = False\n",
    "        self.test_results = []\n",
    "    \n",
    "    def authenticate(self) -> bool:\n",
    "        \"\"\"Autentica na API\"\"\"\n",
    "        url = f\"{self.base_url}/Login/Autenticar?token={self.token}\"\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = self.session.post(url)\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            self.authenticated = response.text.lower() == 'true'\n",
    "            \n",
    "            result = {\n",
    "                'endpoint': 'Login/Autenticar',\n",
    "                'success': self.authenticated,\n",
    "                'status_code': response.status_code,\n",
    "                'latency_ms': round(elapsed * 1000, 2),\n",
    "                'response': response.text\n",
    "            }\n",
    "            self.test_results.append(result)\n",
    "            \n",
    "            return self.authenticated\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro na autenticação: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def test_endpoint(self, endpoint: str, params: Optional[Dict] = None) -> Dict:\n",
    "        \"\"\"Testa um endpoint específico\"\"\"\n",
    "        if not self.authenticated:\n",
    "            print(\"⚠️ Não autenticado! Execute authenticate() primeiro.\")\n",
    "            return None\n",
    "        \n",
    "        url = f\"{self.base_url}/{endpoint}\"\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = self.session.get(url, params=params)\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            try:\n",
    "                data = response.json()\n",
    "            except:\n",
    "                data = response.text\n",
    "            \n",
    "            result = {\n",
    "                'endpoint': endpoint,\n",
    "                'success': response.status_code == 200,\n",
    "                'status_code': response.status_code,\n",
    "                'latency_ms': round(elapsed * 1000, 2),\n",
    "                'data_type': type(data).__name__,\n",
    "                'data_size': len(str(data)),\n",
    "                'response': data\n",
    "            }\n",
    "            \n",
    "            self.test_results.append(result)\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro ao testar {endpoint}: {e}\")\n",
    "            return {'endpoint': endpoint, 'success': False, 'error': str(e)}\n",
    "    \n",
    "    def get_summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Retorna resumo dos testes\"\"\"\n",
    "        df = pd.DataFrame(self.test_results)\n",
    "        return df[['endpoint', 'success', 'status_code', 'latency_ms']]\n",
    "\n",
    "# Inicializar tester\n",
    "api_tester = SPTransAPITester(API_TOKEN, BASE_URL)\n",
    "print(\"✅ API Tester inicializado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Teste de Autenticação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔐 Testando autenticação...\\n\")\n",
    "\n",
    "auth_success = api_tester.authenticate()\n",
    "\n",
    "if auth_success:\n",
    "    print(\"✅ Autenticação bem-sucedida!\")\n",
    "    print(f\"   Status: {api_tester.test_results[-1]['status_code']}\")\n",
    "    print(f\"   Latência: {api_tester.test_results[-1]['latency_ms']} ms\")\n",
    "else:\n",
    "    print(\"❌ Falha na autenticação!\")\n",
    "    print(\"\\nVerifique:\")\n",
    "    print(\"  1. Token está correto\")\n",
    "    print(\"  2. Conexão com a internet\")\n",
    "    print(\"  3. API SPTrans está disponível\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Teste de Endpoints - Linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚌 Testando endpoints de LINHAS...\\n\")\n",
    "\n",
    "# 1. Buscar linhas\n",
    "print(\"1️⃣ Buscar Linhas (termo: 'Centro')\")\n",
    "result = api_tester.test_endpoint('Linha/Buscar', params={'termosBusca': 'Centro'})\n",
    "if result and result['success']:\n",
    "    linhas = result['response']\n",
    "    print(f\"   ✅ Encontradas {len(linhas)} linhas\")\n",
    "    print(f\"   📊 Latência: {result['latency_ms']} ms\")\n",
    "    if linhas:\n",
    "        print(f\"\\n   Exemplo de linha:\")\n",
    "        print(f\"   {json.dumps(linhas[0], indent=2, ensure_ascii=False)}\")\n",
    "else:\n",
    "    print(\"   ❌ Falha ao buscar linhas\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 2. Listar todas as linhas (se disponível)\n",
    "print(\"2️⃣ Listar Todas as Linhas\")\n",
    "result = api_tester.test_endpoint('Linha/Listar')\n",
    "if result and result['success']:\n",
    "    print(f\"   ✅ Total de linhas: {len(result['response'])}\")\n",
    "    print(f\"   📊 Latência: {result['latency_ms']} ms\")\n",
    "else:\n",
    "    print(\"   ℹ️ Endpoint não disponível ou sem dados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Teste de Endpoints - Paradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚏 Testando endpoints de PARADAS...\\n\")\n",
    "\n",
    "# 1. Buscar paradas\n",
    "print(\"1️⃣ Buscar Paradas (termo: 'Paulista')\")\n",
    "result = api_tester.test_endpoint('Parada/Buscar', params={'termosBusca': 'Paulista'})\n",
    "if result and result['success']:\n",
    "    paradas = result['response']\n",
    "    print(f\"   ✅ Encontradas {len(paradas)} paradas\")\n",
    "    print(f\"   📊 Latência: {result['latency_ms']} ms\")\n",
    "    if paradas:\n",
    "        print(f\"\\n   Exemplo de parada:\")\n",
    "        print(f\"   {json.dumps(paradas[0], indent=2, ensure_ascii=False)}\")\n",
    "else:\n",
    "    print(\"   ❌ Falha ao buscar paradas\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 2. Buscar paradas por corredor\n",
    "print(\"2️⃣ Buscar Paradas por Corredor (código: 1)\")\n",
    "result = api_tester.test_endpoint('Parada/BuscarParadasPorCorredor', params={'codigoCorredor': 1})\n",
    "if result and result['success']:\n",
    "    print(f\"   ✅ Paradas encontradas no corredor\")\n",
    "    print(f\"   📊 Latência: {result['latency_ms']} ms\")\n",
    "else:\n",
    "    print(\"   ℹ️ Endpoint não disponível ou sem dados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Teste de Endpoints - Posições em Tempo Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📍 Testando endpoints de POSIÇÕES EM TEMPO REAL...\\n\")\n",
    "\n",
    "# Buscar uma linha primeiro para testar posições\n",
    "linhas_result = api_tester.test_endpoint('Linha/Buscar', params={'termosBusca': '8000'})\n",
    "\n",
    "if linhas_result and linhas_result['success'] and linhas_result['response']:\n",
    "    codigo_linha = linhas_result['response'][0].get('cl', None)\n",
    "    \n",
    "    if codigo_linha:\n",
    "        print(f\"1️⃣ Posições dos Veículos (Linha: {codigo_linha})\")\n",
    "        result = api_tester.test_endpoint('Posicao/Linha', params={'codigoLinha': codigo_linha})\n",
    "        \n",
    "        if result and result['success']:\n",
    "            data = result['response']\n",
    "            if isinstance(data, dict) and 'vs' in data:\n",
    "                veiculos = data['vs']\n",
    "                print(f\"   ✅ {len(veiculos)} veículos ativos\")\n",
    "                print(f\"   📊 Latência: {result['latency_ms']} ms\")\n",
    "                print(f\"   🕐 Horário de referência: {data.get('hr', 'N/A')}\")\n",
    "                \n",
    "                if veiculos:\n",
    "                    print(f\"\\n   Exemplo de veículo:\")\n",
    "                    print(f\"   {json.dumps(veiculos[0], indent=2, ensure_ascii=False)}\")\n",
    "                    \n",
    "                    # Criar DataFrame para análise\n",
    "                    df_veiculos = pd.DataFrame(veiculos)\n",
    "                    print(f\"\\n   📊 Resumo estatístico:\")\n",
    "                    print(f\"   - Veículos únicos: {df_veiculos['p'].nunique() if 'p' in df_veiculos.columns else 'N/A'}\")\n",
    "                    if 'py' in df_veiculos.columns and 'px' in df_veiculos.columns:\n",
    "                        print(f\"   - Lat média: {df_veiculos['py'].mean():.6f}\")\n",
    "                        print(f\"   - Lon média: {df_veiculos['px'].mean():.6f}\")\n",
    "            else:\n",
    "                print(\"   ℹ️ Nenhum veículo ativo no momento\")\n",
    "        else:\n",
    "            print(\"   ❌ Falha ao buscar posições\")\n",
    "    else:\n",
    "        print(\"   ⚠️ Não foi possível obter código da linha\")\n",
    "else:\n",
    "    print(\"   ⚠️ Não foi possível buscar linhas para teste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Teste de Endpoints - Previsão de Chegada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"⏰ Testando endpoints de PREVISÃO DE CHEGADA...\\n\")\n",
    "\n",
    "# Buscar uma parada primeiro\n",
    "paradas_result = api_tester.test_endpoint('Parada/Buscar', params={'termosBusca': 'Paulista'})\n",
    "\n",
    "if paradas_result and paradas_result['success'] and paradas_result['response']:\n",
    "    codigo_parada = paradas_result['response'][0].get('cp', None)\n",
    "    \n",
    "    if codigo_parada:\n",
    "        print(f\"1️⃣ Previsão de Chegada (Parada: {codigo_parada})\")\n",
    "        result = api_tester.test_endpoint('Previsao/Parada', params={'codigoParada': codigo_parada})\n",
    "        \n",
    "        if result and result['success']:\n",
    "            data = result['response']\n",
    "            if isinstance(data, dict):\n",
    "                print(f\"   ✅ Previsões obtidas\")\n",
    "                print(f\"   📊 Latência: {result['latency_ms']} ms\")\n",
    "                print(f\"   🕐 Horário: {data.get('hr', 'N/A')}\")\n",
    "                \n",
    "                if 'p' in data and 'l' in data['p']:\n",
    "                    linhas_previsao = data['p']['l']\n",
    "                    print(f\"   🚌 {len(linhas_previsao)} linhas com previsão\")\n",
    "                    \n",
    "                    if linhas_previsao:\n",
    "                        print(f\"\\n   Exemplo de previsão:\")\n",
    "                        print(f\"   {json.dumps(linhas_previsao[0], indent=2, ensure_ascii=False)}\")\n",
    "        else:\n",
    "            print(\"   ❌ Falha ao buscar previsões\")\n",
    "    else:\n",
    "        print(\"   ⚠️ Não foi possível obter código da parada\")\n",
    "else:\n",
    "    print(\"   ⚠️ Não foi possível buscar paradas para teste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ Teste de Endpoints - Corredores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🛣️ Testando endpoints de CORREDORES...\\n\")\n",
    "\n",
    "print(\"1️⃣ Listar Corredores\")\n",
    "result = api_tester.test_endpoint('Corredor')\n",
    "if result and result['success']:\n",
    "    corredores = result['response']\n",
    "    print(f\"   ✅ {len(corredores)} corredores encontrados\")\n",
    "    print(f\"   📊 Latência: {result['latency_ms']} ms\")\n",
    "    \n",
    "    if corredores:\n",
    "        print(f\"\\n   Corredores disponíveis:\")\n",
    "        for corredor in corredores[:5]:  # Primeiros 5\n",
    "            print(f\"   - {corredor}\")\n",
    "else:\n",
    "    print(\"   ℹ️ Endpoint não disponível ou sem dados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8️⃣ Análise de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo dos testes\n",
    "summary_df = api_tester.get_summary()\n",
    "\n",
    "print(\"📊 RESUMO DOS TESTES\")\n",
    "print(\"=\" * 60)\n",
    "display(summary_df)\n",
    "\n",
    "# Estatísticas\n",
    "total_tests = len(summary_df)\n",
    "successful_tests = summary_df['success'].sum()\n",
    "avg_latency = summary_df['latency_ms'].mean()\n",
    "max_latency = summary_df['latency_ms'].max()\n",
    "min_latency = summary_df['latency_ms'].min()\n",
    "\n",
    "print(f\"\\n📈 ESTATÍSTICAS:\")\n",
    "print(f\"  - Testes executados: {total_tests}\")\n",
    "print(f\"  - Testes bem-sucedidos: {successful_tests} ({successful_tests/total_tests*100:.1f}%)\")\n",
    "print(f\"  - Latência média: {avg_latency:.2f} ms\")\n",
    "print(f\"  - Latência mínima: {min_latency:.2f} ms\")\n",
    "print(f\"  - Latência máxima: {max_latency:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de latências\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Latências por endpoint\n",
    "summary_df.plot(x='endpoint', y='latency_ms', kind='bar', ax=axes[0], color='steelblue', legend=False)\n",
    "axes[0].set_title('Latência por Endpoint', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Endpoint')\n",
    "axes[0].set_ylabel('Latência (ms)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Taxa de sucesso\n",
    "success_counts = summary_df['success'].value_counts()\n",
    "axes[1].pie(success_counts.values, labels=['Sucesso', 'Falha'], autopct='%1.1f%%', \n",
    "            colors=['lightgreen', 'lightcoral'], startangle=90)\n",
    "axes[1].set_title('Taxa de Sucesso dos Testes', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9️⃣ Análise de Qualidade dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✅ ANÁLISE DE QUALIDADE DOS DADOS DA API\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "quality_checks = []\n",
    "\n",
    "for test in api_tester.test_results:\n",
    "    if test.get('success') and 'response' in test:\n",
    "        data = test['response']\n",
    "        \n",
    "        check = {\n",
    "            'endpoint': test['endpoint'],\n",
    "            'has_data': len(str(data)) > 0,\n",
    "            'data_type': type(data).__name__,\n",
    "            'is_empty': len(data) == 0 if isinstance(data, (list, dict)) else False\n",
    "        }\n",
    "        \n",
    "        # Verificações específicas\n",
    "        if isinstance(data, list) and len(data) > 0:\n",
    "            check['count'] = len(data)\n",
    "            check['sample'] = data[0] if data else None\n",
    "        elif isinstance(data, dict):\n",
    "            check['keys'] = list(data.keys())\n",
    "        \n",
    "        quality_checks.append(check)\n",
    "\n",
    "# Exibir análise\n",
    "for check in quality_checks:\n",
    "    print(f\"\\n📋 {check['endpoint']}:\")\n",
    "    print(f\"   - Tipo: {check['data_type']}\")\n",
    "    print(f\"   - Possui dados: {'✅' if check['has_data'] else '❌'}\")\n",
    "    if 'count' in check:\n",
    "        print(f\"   - Quantidade: {check['count']}\")\n",
    "    if 'keys' in check:\n",
    "        print(f\"   - Chaves: {check['keys']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔟 Teste de Stress - Múltiplas Requisições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"⚡ TESTE DE STRESS - Múltiplas Requisições Sequenciais\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Fazer 10 requisições seguidas\n",
    "n_requests = 10\n",
    "stress_results = []\n",
    "\n",
    "print(f\"\\nExecutando {n_requests} requisições...\\n\")\n",
    "\n",
    "for i in range(n_requests):\n",
    "    start = time.time()\n",
    "    result = api_tester.test_endpoint('Linha/Buscar', params={'termosBusca': 'Centro'})\n",
    "    elapsed = (time.time() - start) * 1000\n",
    "    \n",
    "    stress_results.append({\n",
    "        'request': i + 1,\n",
    "        'success': result['success'] if result else False,\n",
    "        'latency_ms': elapsed\n",
    "    })\n",
    "    \n",
    "    print(f\"  Request {i+1}/{n_requests}: {'✅' if result and result['success'] else '❌'} ({elapsed:.2f} ms)\")\n",
    "    time.sleep(0.5)  # Pequeno delay entre requisições\n",
    "\n",
    "# Análise\n",
    "stress_df = pd.DataFrame(stress_results)\n",
    "\n",
    "print(f\"\\n📊 RESULTADOS DO TESTE DE STRESS:\")\n",
    "print(f\"  - Taxa de sucesso: {stress_df['success'].sum()}/{n_requests} ({stress_df['success'].sum()/n_requests*100:.1f}%)\")\n",
    "print(f\"  - Latência média: {stress_df['latency_ms'].mean():.2f} ms\")\n",
    "print(f\"  - Latência mínima: {stress_df['latency_ms'].min():.2f} ms\")\n",
    "print(f\"  - Latência máxima: {stress_df['latency_ms'].max():.2f} ms\")\n",
    "print(f\"  - Desvio padrão: {stress_df['latency_ms'].std():.2f} ms\")\n",
    "\n",
    "# Gráfico\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(stress_df['request'], stress_df['latency_ms'], marker='o', linewidth=2, markersize=8)\n",
    "plt.axhline(y=stress_df['latency_ms'].mean(), color='r', linestyle='--', label=f'Média: {stress_df[\"latency_ms\"].mean():.2f} ms')\n",
    "plt.title('Latência ao Longo das Requisições - Teste de Stress', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Número da Requisição')\n",
    "plt.ylabel('Latência (ms)')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 Exportar Relatório de Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar relatório completo\n",
    "report = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'summary': {\n",
    "        'total_tests': total_tests,\n",
    "        'successful_tests': int(successful_tests),\n",
    "        'success_rate': float(successful_tests/total_tests*100),\n",
    "        'avg_latency_ms': float(avg_latency),\n",
    "        'min_latency_ms': float(min_latency),\n",
    "        'max_latency_ms': float(max_latency)\n",
    "    },\n",
    "    'test_results': api_tester.test_results,\n",
    "    'stress_test': stress_results,\n",
    "    'quality_checks': quality_checks\n",
    "}\n",
    "\n",
    "# Salvar relatório\n",
    "report_path = '../data/samples/api_test_report.json'\n",
    "Path(report_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(report, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print(f\"✅ Relatório salvo em: {report_path}\")\n",
    "\n",
    "# Salvar resumo em CSV\n",
    "csv_path = '../data/samples/api_test_summary.csv'\n",
    "summary_df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ Resumo CSV salvo em: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Conclusões e Recomendações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎯 CONCLUSÕES E RECOMENDAÇÕES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "conclusions = []\n",
    "\n",
    "# Análise de performance\n",
    "if avg_latency < 500:\n",
    "    conclusions.append(\"✅ Performance excelente: latência média abaixo de 500ms\")\n",
    "elif avg_latency < 1000:\n",
    "    conclusions.append(\"⚠️ Performance boa: latência média entre 500-1000ms\")\n",
    "else:\n",
    "    conclusions.append(\"❌ Performance preocupante: latência média acima de 1000ms\")\n",
    "\n",
    "# Análise de confiabilidade\n",
    "success_rate = (successful_tests/total_tests*100)\n",
    "if success_rate >= 95:\n",
    "    conclusions.append(\"✅ Alta confiabilidade: taxa de sucesso >= 95%\")\n",
    "elif success_rate >= 80:\n",
    "    conclusions.append(\"⚠️ Confiabilidade moderada: taxa de sucesso entre 80-95%\")\n",
    "else:\n",
    "    conclusions.append(\"❌ Baixa confiabilidade: taxa de sucesso < 80%\")\n",
    "\n",
    "# Recomendações\n",
    "recommendations = [\n",
    "    \"📌 Implementar retry logic para requisições falhadas\",\n",
    "    \"📌 Adicionar circuit breaker para proteção contra falhas\",\n",
    "    \"📌 Implementar cache para reduzir carga na API\",\n",
    "    \"📌 Monitorar latência e taxa de erro em produção\",\n",
    "    \"📌 Considerar rate limiting para evitar sobrecarga\"\n",
    "]\n",
    "\n",
    "print(\"\\n📊 Conclusões:\")\n",
    "for conclusion in conclusions:\n",
    "    print(f\"  {conclusion}\")\n",
    "\n",
    "print(\"\\n💡 Recomendações:\")\n",
    "for rec in recommendations:\n",
    "    print(f\"  {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✅ Testes da API concluídos com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
