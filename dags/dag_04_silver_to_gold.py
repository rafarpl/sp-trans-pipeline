"""
DAG 04 - Silver to Gold Aggregation
====================================
Agrega√ß√£o de dados da camada Silver para Gold com c√°lculo de KPIs.

Execu√ß√£o: A cada hora
Processamento:
- KPIs por hora e rota
- M√©tricas de performance
- An√°lise de headway
- Estat√≠sticas operacionais

Autor: Rafael - SPTrans Pipeline
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.empty import EmptyOperator
from airflow.sensors.external_task import ExternalTaskSensor

import sys
import os

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.common.config import Config
from src.common.logging_config import get_logger
from src.processing.jobs.silver_to_gold import SilverToGoldJob
from src.common.metrics import reporter, track_job_execution

logger = get_logger(__name__)


# ===========================
# CONFIGURA√á√ïES DO DAG
# ===========================

default_args = {
    'owner': 'sptrans-data-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email': ['alerts@sptrans-pipeline.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2),
}

dag = DAG(
    'dag_04_silver_to_gold',
    default_args=default_args,
    description='Agrega√ß√£o Silver ‚Üí Gold com KPIs e m√©tricas',
    schedule_interval='0 * * * *',  # A cada hora no minuto 0
    catchup=False,
    max_active_runs=1,
    tags=['aggregation', 'silver', 'gold', 'kpis', 'analytics'],
)


# ===========================
# FUN√á√ïES DAS TASKS
# ===========================

@track_job_execution('validate_silver_data')
def validate_silver_availability(**context):
    """
    Task 1: Valida se dados est√£o dispon√≠veis na camada Silver.
    """
    logger.info("Validando disponibilidade de dados na Silver")
    
    from pyspark.sql import SparkSession
    from pyspark.sql import functions as F
    config = Config()
    
    spark = SparkSession.builder \
        .appName("Silver_Data_Validation") \
        .config("spark.driver.memory", "1g") \
        .getOrCreate()
    
    try:
        silver_path = f"{config.SILVER_LAYER_PATH}/positions_enriched"
        
        # Ler dados
        df = spark.read.parquet(silver_path)
        
        # Filtrar √∫ltima hora
        recent_data = df.filter(
            F.col('silver_processing_timestamp') >= F.expr("current_timestamp() - interval 1 hour")
        )
        
        count = recent_data.count()
        
        if count == 0:
            logger.warning("Nenhum dado recente na Silver (√∫ltima hora)")
        else:
            logger.info(f"Dados dispon√≠veis: {count:,} registros na √∫ltima hora")
        
        # Estat√≠sticas b√°sicas
        stats = {
            'total_records': count,
            'unique_vehicles': recent_data.select('vehicle_id').distinct().count(),
            'unique_routes': recent_data.select('route_code').distinct().count(),
            'date_range': {
                'min': recent_data.agg(F.min('timestamp')).collect()[0][0],
                'max': recent_data.agg(F.max('timestamp')).collect()[0][0]
            }
        }
        
        logger.info(f"Estat√≠sticas: {stats}")
        
        # Push para XCom
        context['task_instance'].xcom_push(key='silver_stats', value=stats)
        
        return stats
    
    finally:
        spark.stop()


@track_job_execution('calculate_hourly_kpis')
def calculate_hourly_kpis(**context):
    """
    Task 2: Calcula KPIs por hora.
    
    KPIs incluem:
    - Total de ve√≠culos ativos
    - Ve√≠culos em movimento vs parados
    - Velocidade m√©dia/m√°xima
    - Score de qualidade de dados
    """
    logger.info("Calculando KPIs por hora")
    
    execution_date = context['execution_date']
    config = Config()
    
    from pyspark.sql import SparkSession
    from pyspark.sql import functions as F
    from pyspark.sql.window import Window
    
    spark = SparkSession.builder \
        .appName("Hourly_KPIs_Calculation") \
        .config("spark.driver.memory", "2g") \
        .config("spark.executor.memory", "2g") \
        .getOrCreate()
    
    try:
        # Ler dados da Silver
        silver_path = f"{config.SILVER_LAYER_PATH}/positions_enriched"
        df = spark.read.parquet(silver_path)
        
        # Filtrar √∫ltima hora
        df_hour = df.filter(
            F.col('silver_processing_timestamp') >= F.expr("current_timestamp() - interval 1 hour")
        )
        
        logger.info(f"Processando {df_hour.count():,} registros")
        
        # Agrega√ß√µes por hora e rota
        kpis_hourly = df_hour.groupBy(
            'date',
            'hour',
            'route_code',
            'route_short_name'
        ).agg(
            F.countDistinct('vehicle_id').alias('total_vehicles'),
            F.sum(F.when(F.col('is_moving'), 1).otherwise(0)).alias('vehicles_moving'),
            F.sum(F.when(~F.col('is_moving'), 1).otherwise(0)).alias('vehicles_stopped'),
            F.avg('speed_kmh').alias('avg_speed_kmh'),
            F.max('speed_kmh').alias('max_speed_kmh'),
            F.count('*').alias('total_records'),
            F.avg('data_quality_score').alias('data_quality_score')
        )
        
        # Adicionar timestamp de processamento
        kpis_hourly = kpis_hourly.withColumn(
            'processing_timestamp',
            F.lit(datetime.now())
        )
        
        # Salvar na Gold
        gold_kpis_path = f"{config.GOLD_LAYER_PATH}/kpis_hourly"
        
        kpis_hourly.write \
            .mode('append') \
            .partitionBy('date') \
            .parquet(gold_kpis_path)
        
        kpi_count = kpis_hourly.count()
        logger.info(f"KPIs calculados: {kpi_count:,} registros")
        
        # Push para XCom
        context['task_instance'].xcom_push(
            key='kpis_count',
            value=kpi_count
        )
        
        return {
            'kpis_generated': kpi_count,
            'output_path': gold_kpis_path
        }
    
    finally:
        spark.stop()


@track_job_execution('calculate_route_metrics')
def calculate_route_metrics(**context):
    """
    Task 3: Calcula m√©tricas por rota.
    
    M√©tricas incluem:
    - Total de viagens
    - Dist√¢ncia percorrida
    - Horas de servi√ßo
    - Performance operacional
    """
    logger.info("Calculando m√©tricas por rota")
    
    execution_date = context['execution_date']
    config = Config()
    
    from pyspark.sql import SparkSession
    from pyspark.sql import functions as F
    from pyspark.sql.window import Window
    
    spark = SparkSession.builder \
        .appName("Route_Metrics_Calculation") \
        .config("spark.driver.memory", "2g") \
        .getOrCreate()
    
    try:
        # Ler dados da Silver
        silver_path = f"{config.SILVER_LAYER_PATH}/positions_enriched"
        df = spark.read.parquet(silver_path)
        
        # Filtrar √∫ltimo dia
        df_day = df.filter(
            F.col('date') == F.lit(execution_date.date())
        )
        
        logger.info(f"Processando dados de {execution_date.date()}")
        
        # Calcular dist√¢ncia total por ve√≠culo (soma das dist√¢ncias entre pontos)
        window_vehicle = Window.partitionBy('vehicle_id', 'date').orderBy('timestamp')
        
        df_with_distance = df_day.withColumn(
            'prev_lat',
            F.lag('latitude', 1).over(window_vehicle)
        ).withColumn(
            'prev_lon',
            F.lag('longitude', 1).over(window_vehicle)
        )
        
        # Calcular dist√¢ncia aproximada (em km)
        df_with_distance = df_with_distance.withColumn(
            'distance_km',
            F.when(
                F.col('prev_lat').isNotNull(),
                F.sqrt(
                    F.pow(F.col('latitude') - F.col('prev_lat'), 2) +
                    F.pow(F.col('longitude') - F.col('prev_lon'), 2)
                ) * 111  # Convers√£o para km
            ).otherwise(0)
        )
        
        # Agregar por rota
        route_metrics = df_with_distance.groupBy(
            'date',
            'route_code',
            'route_short_name',
            'route_long_name'
        ).agg(
            F.countDistinct('vehicle_id').alias('total_vehicles'),
            F.count('*').alias('total_observations'),
            F.avg('speed_kmh').alias('avg_speed'),
            F.sum('distance_km').alias('distance_traveled_km'),
            F.countDistinct(
                F.concat_ws('_', 'vehicle_id', 'hour')
            ).alias('vehicle_hours'),
            F.avg('data_quality_score').alias('data_quality_score')
        )
        
        # Calcular horas de servi√ßo (aproximado)
        route_metrics = route_metrics.withColumn(
            'service_hours',
            F.col('vehicle_hours') / F.col('total_vehicles')
        )
        
        # Adicionar timestamp
        route_metrics = route_metrics.withColumn(
            'processing_timestamp',
            F.lit(datetime.now())
        )
        
        # Salvar na Gold
        gold_metrics_path = f"{config.GOLD_LAYER_PATH}/metrics_by_route"
        
        route_metrics.write \
            .mode('append') \
            .partitionBy('date') \
            .parquet(gold_metrics_path)
        
        metrics_count = route_metrics.count()
        logger.info(f"M√©tricas de rota calculadas: {metrics_count:,} rotas")
        
        return {
            'routes_processed': metrics_count,
            'output_path': gold_metrics_path
        }
    
    finally:
        spark.stop()


@track_job_execution('calculate_headway_analysis')
def calculate_headway_analysis(**context):
    """
    Task 4: An√°lise de headway (intervalo entre √¥nibus).
    
    Headway √© o tempo entre a passagem de dois √¥nibus consecutivos
    da mesma linha em uma parada.
    """
    logger.info("Calculando an√°lise de headway")
    
    execution_date = context['execution_date']
    config = Config()
    
    from pyspark.sql import SparkSession
    from pyspark.sql import functions as F
    from pyspark.sql.window import Window
    
    spark = SparkSession.builder \
        .appName("Headway_Analysis") \
        .config("spark.driver.memory", "2g") \
        .getOrCreate()
    
    try:
        # Ler dados da Silver
        silver_path = f"{config.SILVER_LAYER_PATH}/positions_enriched"
        df = spark.read.parquet(silver_path)
        
        # Filtrar √∫ltimo dia
        df_day = df.filter(
            F.col('date') == F.lit(execution_date.date())
        )
        
        # Para cada rota e hora, calcular intervalo entre ve√≠culos
        window_headway = Window.partitionBy('route_code', 'hour').orderBy('timestamp')
        
        df_headway = df_day.withColumn(
            'prev_timestamp',
            F.lag('timestamp', 1).over(window_headway)
        ).withColumn(
            'prev_vehicle',
            F.lag('vehicle_id', 1).over(window_headway)
        )
        
        # Calcular headway em minutos
        df_headway = df_headway.withColumn(
            'headway_minutes',
            F.when(
                (F.col('prev_timestamp').isNotNull()) & 
                (F.col('vehicle_id') != F.col('prev_vehicle')),
                (F.unix_timestamp('timestamp') - F.unix_timestamp('prev_timestamp')) / 60.0
            )
        ).filter(F.col('headway_minutes').isNotNull())
        
        # Agregar estat√≠sticas de headway
        headway_stats = df_headway.groupBy(
            'date',
            'hour',
            'route_code',
            'route_short_name'
        ).agg(
            F.avg('headway_minutes').alias('avg_headway_minutes'),
            F.min('headway_minutes').alias('min_headway_minutes'),
            F.max('headway_minutes').alias('max_headway_minutes'),
            F.stddev('headway_minutes').alias('headway_std_dev'),
            F.count('*').alias('observations')
        )
        
        # Calcular vari√¢ncia de headway (regularidade do servi√ßo)
        headway_stats = headway_stats.withColumn(
            'headway_variance',
            F.col('headway_std_dev') / F.col('avg_headway_minutes')
        )
        
        # Adicionar timestamp
        headway_stats = headway_stats.withColumn(
            'processing_timestamp',
            F.lit(datetime.now())
        )
        
        # Salvar na Gold
        gold_headway_path = f"{config.GOLD_LAYER_PATH}/headway_analysis"
        
        headway_stats.write \
            .mode('append') \
            .partitionBy('date') \
            .parquet(gold_headway_path)
        
        headway_count = headway_stats.count()
        logger.info(f"An√°lise de headway conclu√≠da: {headway_count:,} registros")
        
        return {
            'headway_records': headway_count,
            'output_path': gold_headway_path
        }
    
    finally:
        spark.stop()


def generate_summary_statistics(**context):
    """
    Task 5: Gera estat√≠sticas resumidas.
    """
    logger.info("Gerando estat√≠sticas resumidas")
    
    ti = context['task_instance']
    execution_date = context['execution_date']
    
    # Coletar dados das tasks anteriores
    silver_stats = ti.xcom_pull(task_ids='validate_silver_availability', key='silver_stats')
    kpis_count = ti.xcom_pull(task_ids='calculate_hourly_kpis', key='kpis_count')
    
    summary = {
        'execution_date': execution_date.isoformat(),
        'silver_records_processed': silver_stats.get('total_records', 0),
        'unique_vehicles': silver_stats.get('unique_vehicles', 0),
        'unique_routes': silver_stats.get('unique_routes', 0),
        'kpis_generated': kpis_count,
        'timestamp': datetime.now().isoformat()
    }
    
    logger.info(f"Resumo: {summary}")
    
    # Reportar m√©tricas
    reporter.report_processing_stats(
        layer='gold',
        source='aggregations',
        total=kpis_count,
        invalid=0,
        duplicated=0
    )
    
    return summary


def send_completion_notification(**context):
    """
    Task 6: Notifica√ß√£o de conclus√£o.
    """
    logger.info("Enviando notifica√ß√£o de conclus√£o")
    
    ti = context['task_instance']
    execution_date = context['execution_date']
    
    silver_stats = ti.xcom_pull(task_ids='validate_silver_availability', key='silver_stats')
    kpis_count = ti.xcom_pull(task_ids='calculate_hourly_kpis', key='kpis_count')
    
    message = f"""
    ‚úÖ DAG Silver ‚Üí Gold Conclu√≠da
    
    Data: {execution_date}
    
    üìä Processamento:
    - Registros Silver: {silver_stats.get('total_records', 0):,}
    - Ve√≠culos √∫nicos: {silver_stats.get('unique_vehicles', 0):,}
    - Rotas √∫nicas: {silver_stats.get('unique_routes', 0):,}
    
    üìà Agrega√ß√µes Geradas:
    - KPIs por hora: {kpis_count:,}
    - M√©tricas de rota: Calculadas
    - An√°lise de headway: Calculada
    
    Status: SUCCESS ‚úÖ
    """
    
    logger.info(message)
    
    return message


# ===========================
# DEFINI√á√ÉO DAS TASKS
# ===========================

with dag:
    
    start = EmptyOperator(
        task_id='start',
        dag=dag
    )
    
    # Task 1: Valida√ß√£o
    validate_task = PythonOperator(
        task_id='validate_silver_availability',
        python_callable=validate_silver_availability,
        provide_context=True,
        dag=dag
    )
    
    # Task 2: KPIs por hora
    kpis_task = PythonOperator(
        task_id='calculate_hourly_kpis',
        python_callable=calculate_hourly_kpis,
        provide_context=True,
        dag=dag
    )
    
    # Task 3: M√©tricas por rota
    metrics_task = PythonOperator(
        task_id='calculate_route_metrics',
        python_callable=calculate_route_metrics,
        provide_context=True,
        dag=dag
    )
    
    # Task 4: An√°lise de headway
    headway_task = PythonOperator(
        task_id='calculate_headway_analysis',
        python_callable=calculate_headway_analysis,
        provide_context=True,
        dag=dag
    )
    
    # Task 5: Estat√≠sticas
    summary_task = PythonOperator(
        task_id='generate_summary_statistics',
        python_callable=generate_summary_statistics,
        provide_context=True,
        dag=dag
    )
    
    # Task 6: Notifica√ß√£o
    notify_task = PythonOperator(
        task_id='send_notification',
        python_callable=send_completion_notification,
        provide_context=True,
        trigger_rule='all_done',
        dag=dag
    )
    
    end = EmptyOperator(
        task_id='end',
        trigger_rule='all_done',
        dag=dag
    )
    
    # ===========================
    # FLUXO
    # ===========================
    
    # Valida√ß√£o primeiro
    start >> validate_task
    
    # Depois executa agrega√ß√µes em paralelo
    validate_task >> [kpis_task, metrics_task, headway_task]
    
    # Estat√≠sticas e notifica√ß√£o
    [kpis_task, metrics_task, headway_task] >> summary_task >> notify_task >> end


# ===========================
# DOCUMENTA√á√ÉO
# ===========================

dag.doc_md = """
# DAG 04 - Silver to Gold Aggregation

## Objetivo
Agregar dados limpos da camada Silver em KPIs e m√©tricas de neg√≥cio na camada Gold.

## Schedule
- **Frequ√™ncia**: A cada hora (no minuto 0)
- **Depend√™ncias**: DAG 03 (Bronze to Silver)

## Agrega√ß√µes Calculadas

### 1. KPIs por Hora
- Total de ve√≠culos ativos
- Ve√≠culos em movimento vs parados
- Velocidade m√©dia e m√°xima
- Score de qualidade

### 2. M√©tricas por Rota
- Total de viagens
- Dist√¢ncia percorrida
- Horas de servi√ßo
- Performance operacional

### 3. An√°lise de Headway
- Intervalo m√©dio entre √¥nibus
- Regularidade do servi√ßo
- Vari√¢ncia de headway

## Fluxo

```
Start ‚Üí Validate Silver ‚Üí [KPIs | Metrics | Headway] ‚Üí Summary ‚Üí Notify ‚Üí End
```

## Uso das Agrega√ß√µes
- Dashboards executivos
- An√°lises de performance
- Planejamento operacional
- Relat√≥rios gerenciais

## Monitoramento
- M√©tricas em Prometheus
- Dashboards em Grafana/Superset
- Alertas se processamento falhar
"""
