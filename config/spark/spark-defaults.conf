# ============================================================================
# SPARK DEFAULT CONFIGURATION
# ============================================================================
# Localização: config/spark/spark-defaults.conf
# ============================================================================

# ============================================================================
# SPARK CORE
# ============================================================================
spark.master                     spark://spark-master:7077
spark.app.name                   SPTransPipeline

# Driver
spark.driver.memory              2g
spark.driver.cores               1
spark.driver.maxResultSize       1g

# Executor
spark.executor.memory            4g
spark.executor.cores             2
spark.executor.instances         2

# ============================================================================
# SHUFFLE & PARALLELISM
# ============================================================================
spark.default.parallelism        12
spark.sql.shuffle.partitions     8
spark.shuffle.compress           true
spark.shuffle.spill.compress     true

# ============================================================================
# ADAPTIVE QUERY EXECUTION (AQE)
# ============================================================================
spark.sql.adaptive.enabled                      true
spark.sql.adaptive.coalescePartitions.enabled   true
spark.sql.adaptive.skewJoin.enabled             true

# ============================================================================
# DELTA LAKE
# ============================================================================
spark.sql.extensions                io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog     org.apache.spark.sql.delta.catalog.DeltaCatalog

# ============================================================================
# S3/MINIO CONFIGURATION
# ============================================================================
spark.hadoop.fs.s3a.impl                    org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.endpoint                http://minio:9000
spark.hadoop.fs.s3a.access.key              minioadmin
spark.hadoop.fs.s3a.secret.key              minioadmin123
spark.hadoop.fs.s3a.path.style.access       true
spark.hadoop.fs.s3a.connection.ssl.enabled  false
spark.hadoop.fs.s3a.fast.upload             true
spark.hadoop.fs.s3a.block.size              128M
spark.hadoop.fs.s3a.multipart.size          104857600
spark.hadoop.fs.s3a.connection.maximum      100

# ============================================================================
# MEMORY MANAGEMENT
# ============================================================================
spark.memory.fraction            0.6
spark.memory.storageFraction     0.5
spark.memory.offHeap.enabled     false

# ============================================================================
# SERIALIZATION
# ============================================================================
spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.kryoserializer.buffer.max  256m

# ============================================================================
# DYNAMIC ALLOCATION (desabilitado para cluster fixo)
# ============================================================================
spark.dynamicAllocation.enabled  false

# ============================================================================
# LOGGING
# ============================================================================
spark.eventLog.enabled           false
spark.ui.enabled                 true
spark.ui.port                    4040

# ============================================================================
# NETWORK
# ============================================================================
spark.network.timeout            600s
spark.executor.heartbeatInterval 60s

# ============================================================================
# CHECKPOINTING
# ============================================================================
spark.cleaner.referenceTracking.cleanCheckpoints  true

# ============================================================================
# FIM DO ARQUIVO
# ============================================================================