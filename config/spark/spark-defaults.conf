#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# ============================================================================
# SPTRANS PIPELINE - SPARK CONFIGURATION
# ============================================================================
# Configurações otimizadas para processamento de dados do SPTrans
# ============================================================================

# ----------------------------------------------------------------------------
# BASIC SETTINGS
# ----------------------------------------------------------------------------
spark.master                                spark://spark-master:7077
spark.app.name                              SPTransPipeline
spark.submit.deployMode                     client

# ----------------------------------------------------------------------------
# EXECUTOR SETTINGS
# ----------------------------------------------------------------------------
# Configurações de executores para otimizar performance
spark.executor.instances                    2
spark.executor.cores                        2
spark.executor.memory                       4g
spark.executor.memoryOverhead              1g

# ----------------------------------------------------------------------------
# DRIVER SETTINGS
# ----------------------------------------------------------------------------
spark.driver.cores                          2
spark.driver.memory                         2g
spark.driver.memoryOverhead                512m
spark.driver.maxResultSize                  1g

# ----------------------------------------------------------------------------
# MEMORY MANAGEMENT
# ----------------------------------------------------------------------------
# Configurações de memória para otimizar uso de recursos
spark.memory.fraction                       0.6
spark.memory.storageFraction               0.5
spark.memory.offHeap.enabled               true
spark.memory.offHeap.size                  2g

# ----------------------------------------------------------------------------
# SHUFFLE SETTINGS
# ----------------------------------------------------------------------------
# Otimizações para operações de shuffle
spark.shuffle.service.enabled              true
spark.shuffle.compress                     true
spark.shuffle.spill.compress               true
spark.shuffle.file.buffer                  64k
spark.reducer.maxSizeInFlight              96m
spark.reducer.maxReqsInFlight              5

# ----------------------------------------------------------------------------
# SERIALIZATION
# ----------------------------------------------------------------------------
# Usar Kryo para melhor performance
spark.serializer                           org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired            false
spark.kryoserializer.buffer.max            512m

# ----------------------------------------------------------------------------
# NETWORK
# ----------------------------------------------------------------------------
spark.network.timeout                      300s
spark.rpc.askTimeout                       300s
spark.rpc.lookupTimeout                    300s

# ----------------------------------------------------------------------------
# SQL SETTINGS
# ----------------------------------------------------------------------------
# Configurações para Spark SQL otimizadas
spark.sql.adaptive.enabled                 true
spark.sql.adaptive.coalescePartitions.enabled  true
spark.sql.adaptive.skewJoin.enabled        true
spark.sql.autoBroadcastJoinThreshold       10485760
spark.sql.shuffle.partitions               200
spark.sql.files.maxPartitionBytes          134217728
spark.sql.legacy.parquet.int96AsTimestamp  true
spark.sql.parquet.compression.codec        snappy
spark.sql.parquet.mergeSchema              false
spark.sql.parquet.filterPushdown           true

# ----------------------------------------------------------------------------
# DYNAMIC ALLOCATION
# ----------------------------------------------------------------------------
# Alocação dinâmica de recursos
spark.dynamicAllocation.enabled            true
spark.dynamicAllocation.shuffleTracking.enabled  true
spark.dynamicAllocation.minExecutors       1
spark.dynamicAllocation.maxExecutors       4
spark.dynamicAllocation.initialExecutors   2
spark.dynamicAllocation.executorIdleTimeout     60s
spark.dynamicAllocation.cachedExecutorIdleTimeout  300s

# ----------------------------------------------------------------------------
# S3/MINIO CONFIGURATION
# ----------------------------------------------------------------------------
# Configurações para acesso ao MinIO (S3-compatible)
spark.hadoop.fs.s3a.impl                   org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.endpoint               http://minio:9000
spark.hadoop.fs.s3a.access.key             admin
spark.hadoop.fs.s3a.secret.key             miniopassword123
spark.hadoop.fs.s3a.path.style.access      true
spark.hadoop.fs.s3a.connection.ssl.enabled false
spark.hadoop.fs.s3a.attempts.maximum       10
spark.hadoop.fs.s3a.connection.establish.timeout  5000
spark.hadoop.fs.s3a.connection.timeout     200000
spark.hadoop.fs.s3a.fast.upload            true
spark.hadoop.fs.s3a.multipart.size         104857600
spark.hadoop.fs.s3a.threads.max            20
spark.hadoop.fs.s3a.threads.core           5

# ----------------------------------------------------------------------------
# LOGGING
# ----------------------------------------------------------------------------
spark.eventLog.enabled                     true
spark.eventLog.dir                         /tmp/spark-events
spark.history.fs.logDirectory              /tmp/spark-events
spark.ui.reverseProxy                      true
spark.ui.reverseProxyUrl                   /

# ----------------------------------------------------------------------------
# MONITORING & METRICS
# ----------------------------------------------------------------------------
spark.metrics.conf                         /opt/spark/conf/metrics.properties
spark.ui.prometheus.enabled                true

# ----------------------------------------------------------------------------
# COMPRESSION
# ----------------------------------------------------------------------------
spark.io.compression.codec                 snappy
spark.rdd.compress                         true

# ----------------------------------------------------------------------------
# SPECULATION
# ----------------------------------------------------------------------------
# Evitar problemas com tasks lentas
spark.speculation                          true
spark.speculation.interval                 100ms
spark.speculation.multiplier               1.5
spark.speculation.quantile                 0.75

# ----------------------------------------------------------------------------
# BLACKLISTING
# ----------------------------------------------------------------------------
# Desabilitar executors problemáticos
spark.blacklist.enabled                    true
spark.blacklist.timeout                    1h

# ----------------------------------------------------------------------------
# CHECKPOINTING
# ----------------------------------------------------------------------------
spark.checkpoint.compress                  true

# ----------------------------------------------------------------------------
# CUSTOM SETTINGS - SPTRANS PIPELINE
# ----------------------------------------------------------------------------
# Configurações específicas do projeto
spark.app.id                               sptrans-pipeline-v1
spark.sql.session.timeZone                 America/Sao_Paulo

# ----------------------------------------------------------------------------
# DELTA LAKE SETTINGS (if using)
# ----------------------------------------------------------------------------
# spark.sql.extensions                      io.delta.sql.DeltaSparkSessionExtension
# spark.sql.catalog.spark_catalog           org.apache.spark.sql.delta.catalog.DeltaCatalog

# ----------------------------------------------------------------------------
# ICEBERG SETTINGS (if using)
# ----------------------------------------------------------------------------
# spark.sql.extensions                      org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
# spark.sql.catalog.spark_catalog           org.apache.iceberg.spark.SparkSessionCatalog
# spark.sql.catalog.spark_catalog.type      hive

# ============================================================================
# END OF CONFIGURATION
# ============================================================================
